{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ac06ab3-84d8-460b-843a-31c185710fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../../src/lisfloodreservoirs/fit_starfit.py\n",
    "#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "170df219-eb4a-4aa2-99c9-0a324f61a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from lisfloodreservoirs import Config, read_attributes, read_timeseries\n",
    "from lisfloodreservoirs.models.starfit.storage import fit_storage, create_storage_harmonic\n",
    "from lisfloodreservoirs.models.starfit.release import fit_release, create_release_harmonic, create_release_linear\n",
    "from lisfloodreservoirs.models.starfit.functions import plot_nor, plot_release\n",
    "from lisfloodreservoirs.utils.logging import setup_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c94eb20-1dad-4ef6-9b29-14a02f4a01c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "\n",
    "#     # parse arguments\n",
    "#     parser = argparse.ArgumentParser(\n",
    "#         description=\"\"\"\n",
    "#         Fit the storage and release rules for the Starfit reservoir routine.\n",
    "#         The fitted models are saved as Pickle files and plotted against the\n",
    "#         observed data used for fitting.\n",
    "#         \"\"\"\n",
    "#         )\n",
    "#     parser.add_argument('-c', '--config-file', type=str, required=True, help='Path to the configuration file')\n",
    "#     parser.add_argument('-o', '--overwrite', action='store_true', default=False, help='Overwrite existing model.')\n",
    "#     args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e90348f-d1aa-4104-ba3d-420b5005e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'Z:/nahaUsers/casadje/datasets/reservoirs/ResOpsUS/v2.1/results/starfit/starfit.yml'\n",
    "overwrite = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61c31aac-f5d5-46c8-9b7c-77eba5e67942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 11:06:13 | INFO | __main__ | Storage models will be saved in: Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\v2.1\\results\\starfit\\NOR\n",
      "2025-05-29 11:06:13 | INFO | __main__ | Release models will be saved in: Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\v2.1\\results\\starfit\\release\n",
      "2025-05-29 11:06:13 | INFO | __main__ | 164 reservoirs in the attribute tables\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fba17fee2b4c5c97d0bc4e8ca0576d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 11:06:49 | INFO | __main__ | 164 reservoirs with timeseries\n"
     ]
    }
   ],
   "source": [
    "# set up logger\n",
    "logger = setup_logger(\n",
    "    name=__name__,\n",
    "    log_level=logging.INFO,\n",
    "    log_file=f'{datetime.now():%Y%m%d%H%M%S}_fit_starfit.log'\n",
    ")\n",
    "\n",
    "# read configuration file\n",
    "cfg = Config(config_file)\n",
    "PATH_STORAGE = cfg.PATH_DEF.parent / 'NOR'\n",
    "PATH_RELEASE = cfg.PATH_DEF.parent / 'release'\n",
    "PATH_STORAGE.mkdir(parents=True, exist_ok=True)\n",
    "PATH_RELEASE.mkdir(parents=True, exist_ok=True)\n",
    "logger.info(f'Storage models will be saved in: {PATH_STORAGE}')\n",
    "logger.info(f'Release models will be saved in: {PATH_RELEASE}')\n",
    "\n",
    "# === Load reservoir list ===\n",
    "try:\n",
    "    reservoirs = pd.read_csv(cfg.RESERVOIRS_FILE, header=None).squeeze().tolist()\n",
    "except IOError:\n",
    "    logger.exception(f'Failed to open {cfg.RESERVOIRS_FILE}')\n",
    "    raise\n",
    "\n",
    "# === Load attributes ===\n",
    "try:\n",
    "    attributes = read_attributes(cfg.PATH_DATA / 'attributes', reservoirs)\n",
    "    logger.info(f'{attributes.shape[0]} reservoirs in the attribute tables')\n",
    "except IOError:\n",
    "    logger.exception(f'Failed to read attribute tables')\n",
    "    raise\n",
    "\n",
    "# === Load time periods ===\n",
    "try:\n",
    "    with open(cfg.PERIODS_FILE, 'rb') as file:\n",
    "        periods = pickle.load(file)\n",
    "except IOError:\n",
    "    logger.exception(f'Failed to open {cfg.PERIODS_FILE}')\n",
    "    raise\n",
    "\n",
    "# === read time series ===\n",
    "try:\n",
    "    inputs = [var for var in [cfg.INFLOW, cfg.PRECIPITATION, cfg.EVAPORATION, cfg.DEMAND] if var]\n",
    "    outputs = ['storage', 'outflow']\n",
    "    timeseries = read_timeseries(\n",
    "        path=cfg.PATH_DATA / 'time_series' / 'csv',\n",
    "        reservoirs=attributes.index,\n",
    "        periods=periods,\n",
    "        variables=inputs + outputs\n",
    "    )\n",
    "    for grand_id, obs in timeseries.items():\n",
    "        # convert units\n",
    "        obs['s'] = obs.storage * 1e-6 # MCM\n",
    "        obs[['i', 'r']] = obs[['inflow', 'outflow']] * 1e-6 * 86400 # MCM/day\n",
    "        # update reservoir capacity, if maximum observation exceeds GRanD\n",
    "        attributes.loc[grand_id, 'CAP_MCM'] = max(attributes.loc[grand_id, 'CAP_MCM'], obs.s.max())\n",
    "    logger.info(f'{len(timeseries)} reservoirs with timeseries')\n",
    "except IOError:\n",
    "    logger.exception(f'Failed to read time series')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef300743-7558-477d-b867-d5386298561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fit Storage Models ===\n",
    "for grand_id, obs in tqdm(timeseries.items(), desc=\"Fitting storage models\"):\n",
    "    \n",
    "    out_file = PATH_STORAGE / f'{grand_id}.pkl'\n",
    "    if out_file.exists() and not overwrite:\n",
    "        logger.info(f'Storage model already exists for {grand_id}, skipping (use --overwrite to force)')\n",
    "        continue\n",
    "\n",
    "    logger.info(f'Fitting storage model for {grand_id}')\n",
    "\n",
    "    # fit storage model\n",
    "    try:\n",
    "        for years, n_points in zip([8, 6, 4], [3, 2, 2]):\n",
    "            model_storage = fit_storage(\n",
    "                grand_id,\n",
    "                storage_daily=obs.s,\n",
    "                attributes=attributes.loc[grand_id],\n",
    "                min_days=years * 365,\n",
    "                n_points=n_points,\n",
    "            )\n",
    "            if not model_storage['weekly storage'].empty:\n",
    "                break\n",
    "\n",
    "        if model_storage['weekly storage'].empty:\n",
    "            logger.warning(f'Could not fit storage model for {grand_id}')\n",
    "            continue\n",
    "            \n",
    "    except Exception:\n",
    "        logger.exception(f'Failed to fit storage model for {grand_id}')\n",
    "        continue\n",
    "\n",
    "    # export fitted parameters\n",
    "    try:\n",
    "        pars_to_export = ['capacity (MCM)', 'NOR upper bound', 'NOR lower bound']\n",
    "        pars = {key: value for key, value in model_storage.items() if key in pars_to_export}\n",
    "        with open(out_file, 'wb') as file:\n",
    "            pickle.dump(pars, file)\n",
    "        logger.debug(f'Saved storage model for {grand_id}')\n",
    "    except Exception as e:\n",
    "        logger.exception(f'Failed to save the storage model for {grand_id}')\n",
    "\n",
    "    # define normal operating range (NOR)\n",
    "    try:\n",
    "        NORup = create_storage_harmonic(model_storage['NOR upper bound'], name='flood').set_index('epiweek')\n",
    "        NORdown = create_storage_harmonic(model_storage['NOR lower bound'], name='conservation').set_index('epiweek')\n",
    "        NOR = pd.concat((NORup, NORdown), axis=1)\n",
    "    except Exception as e:\n",
    "        logger.exception(f'Failed to create the normal operating rules (NOR) for {grand_id}')\n",
    "\n",
    "    # weekly time series of standardised storage combined with NOR\n",
    "    weekly_storage = model_storage['weekly storage']\n",
    "\n",
    "    # plot model\n",
    "    try:\n",
    "        plot_nor(\n",
    "            weekly_storage,\n",
    "            NOR,\n",
    "            n_points=n_points,\n",
    "            title='{0} - {1}'.format(grand_id, attributes.loc[grand_id, 'DAM_NAME']),\n",
    "            save=PATH_STORAGE / f'{grand_id}.jpg'\n",
    "        )\n",
    "    except Exception:\n",
    "        logger.exception(f'Failed to plot the storage model of reservoir {grand_id}')\n",
    "\n",
    "# === Fit Release Models ===\n",
    "grand_ids = [int(file.stem) for file in PATH_STORAGE.glob('*.pkl')]\n",
    "for grand_id, obs in tqdm(timeseries.items(), desc=\"Fitting storage models\"):\n",
    "\n",
    "    if grand_id not in grand_ids:\n",
    "        logger.info(f\"Skipping {grand_id} as it doesn't have a storage model\")\n",
    "        \n",
    "    out_file = PATH_RELEASE / f'{grand_id}.pkl'\n",
    "    if out_file.exists() and not args.overwrite:\n",
    "        logger.info(f'Release model already exists for {grand_id}, skipping (use --overwrite to force)')\n",
    "        continue\n",
    "\n",
    "    logger.info(f'Fitting release model for {grand_id}')\n",
    "\n",
    "    # fit release model\n",
    "    try:\n",
    "        for years in [5, 4]:\n",
    "            model_release = fit_release(\n",
    "                grand_id,\n",
    "                daily_ops=obs[['s', 'i', 'r']],\n",
    "                attributes=attributes.loc[grand_id],\n",
    "                NOR_path=PATH_STORAGE,\n",
    "                cutoff_year=None,\n",
    "                min_weeks=52 * years\n",
    "            )\n",
    "            if pd.notna(model_release['mean inflow (MCM/wk)']):\n",
    "                break\n",
    "\n",
    "        if not model_release or all(np.isnan(model_release['harmonic parameters'])):\n",
    "            logger.warning(f'Could not fit release model for {grand_id}')\n",
    "            continue\n",
    "\n",
    "    except Exception:\n",
    "        logger.exception(f'Failed to fit release model for {grand_id}')\n",
    "\n",
    "    # export fitted parameters\n",
    "    try:\n",
    "        pars_to_export = ['mean inflow (MCM/wk)', 'harmonic parameters', 'residual parameters', 'constraints']\n",
    "        pars = {key: value for key, value in model_release.items() if key in pars_to_export}\n",
    "        with open(out_file, 'wb') as file:\n",
    "            pickle.dump(pars, file)\n",
    "    except Exception:\n",
    "        logger.exception(f'Failed to save the release model for {grand_id}')\n",
    "\n",
    "    # extract info from the fitted release: average inflow, harmonic release (standardised) and release contraints\n",
    "    try:\n",
    "        avg_inflow = model_release['mean inflow (MCM/wk)']\n",
    "        release_harmonic = create_release_harmonic(model_release['harmonic parameters']).set_index('epiweek').squeeze()\n",
    "        release_linear = create_release_linear(model_release['residual parameters'])\n",
    "        Qmin, Qmax = model_release['constraints']\n",
    "        weekly_release = model_release['weekly release'].set_index('epiweek')\n",
    "    except Exception as e:\n",
    "        logger.exception(f'Failed to create the release rules for {grand_id}')\n",
    "\n",
    "    # plot model\n",
    "    try:\n",
    "        title = '{0} - {1}'.format(grand_id, attributes.loc[grand_id, 'DAM_NAME'])\n",
    "        plot_release(\n",
    "            weekly_release.r, \n",
    "            avg_inflow, \n",
    "            release_harmonic, \n",
    "            release_linear, \n",
    "            Qmin, \n",
    "            Qmax, \n",
    "            title=title,\n",
    "            save=PATH_RELEASE / f'{grand_id}.jpg'\n",
    "        )\n",
    "    except Exception:\n",
    "        logger.exception(f'Failed to plot the release model for {grand_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e447f576-7463-459c-a4b6-2f09a5539b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51444249-1089-4860-aa95-ae2ef301e215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
