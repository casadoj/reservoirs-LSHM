{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4516f5e0-f0dd-4bdb-af93-3cf34cfb94c6",
   "metadata": {},
   "source": [
    "# ResOpsNO: time series\n",
    "***\n",
    "\n",
    "**Autor:** Chus Casado<br>\n",
    "**Date:** 29-11-2024<br>\n",
    "\n",
    "**Introduction:**<br>\n",
    "\n",
    "**To do:**<br>\n",
    "* [x] Plot time series\n",
    "* [x] Make sure that there aren't negative values in the time series, nor zeros in storage.\n",
    "* [ ] Check the quality of the data by closing the mass balance when possible. <font color='steelblue'>I've used the mass balance to identify errors in the inflow time series (function `clean_inflow`).</font>.\n",
    "* [ ] Fill in the inflow time series with the mass balance, if possible. <font color='steelblue'>I've filled in gaps in the inflow time series with linear interpolation up to 7-day gaps (function `clean_inflow`).</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93f56e74-ae31-4b60-be69-fc127719c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# from datetime import datetime, timedelta\n",
    "from tqdm.auto import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "from reservoirs_lshm.utils import DatasetConfig\n",
    "from reservoirs_lshm import read_attributes\n",
    "from reservoirs_lshm.utils.plots import plot_resops, reservoir_analysis, compare_flows\n",
    "from reservoirs_lshm.utils.timeseries import clean_storage, clean_inflow, time_encoding, quantile_mapping\n",
    "\n",
    "# from utils_br import plot_timeseries_BR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c16a79-1c1c-42d0-8b03-11e6ece2dfb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9034a7b1-74d9-47ea-8457-7f748a3eb4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time series will be saved in Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsNO\\v1.0\\time_series\n"
     ]
    }
   ],
   "source": [
    "cfg = DatasetConfig('config_dataset.yml')\n",
    "\n",
    "print(f'Time series will be saved in {cfg.PATH_TS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f351856-cee6-4d5e-b561-724f4ea6ebb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cbf891-79d3-4317-9964-b99f1df92649",
   "metadata": {},
   "source": [
    "### Attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "614adf34-bc72-4f25-959b-5dab55da8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import all tables of attributes\n",
    "# attributes = read_attributes(cfg.PATH_ATTRS)\n",
    "# map_ana_grand = {sar_id: grand_id for grand_id, sar_id in attributes['SAR_ID'].iteritems()}\n",
    "# print(f'{attributes.shape[0]} reservoirs in the attribute tables')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1228fdc-7588-4e0f-ad5e-25b95df3412a",
   "metadata": {},
   "source": [
    "### Time series\n",
    "#### NVE\n",
    "\n",
    "**Storage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b484bdd-6c23-4d93-b888-0cc91f313cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('Z:/nahaUsers/casadje/datasets/reservoirs/ResOpsNO/raw/timeseries_storage')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.PATH_RESOPS / 'raw' / 'timeseries_storage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50320a56-eeb6-4fdb-80c0-58a9d02fe1b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\DEV\\Anaconda3\\envs\\xr\\lib\\site-packages\\xarray\\backends\\file_manager.py:199\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\DEV\\Anaconda3\\envs\\xr\\lib\\site-packages\\xarray\\backends\\lru_cache.py:53\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m---> 53\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('Z:\\\\nahaUsers\\\\casadje\\\\datasets\\\\reservoirs\\\\ResOpsNO\\\\raw\\\\timeseries_storage\\\\reservoir_time_series.h5',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False))]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPATH_RESOPS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimeseries_storage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreservoir_time_series.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\DEV\\Anaconda3\\envs\\xr\\lib\\site-packages\\xarray\\backends\\api.py:495\u001b[0m, in \u001b[0;36mopen_dataset\u001b[1;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, backend_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    483\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[0;32m    484\u001b[0m     decode_cf,\n\u001b[0;32m    485\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    491\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[0;32m    492\u001b[0m )\n\u001b[0;32m    494\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 495\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mopen_dataset(\n\u001b[0;32m    496\u001b[0m     filename_or_obj,\n\u001b[0;32m    497\u001b[0m     drop_variables\u001b[38;5;241m=\u001b[39mdrop_variables,\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecoders,\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    500\u001b[0m )\n\u001b[0;32m    501\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[0;32m    502\u001b[0m     backend_ds,\n\u001b[0;32m    503\u001b[0m     filename_or_obj,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    511\u001b[0m )\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[1;32mC:\\DEV\\Anaconda3\\envs\\xr\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:550\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[1;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    531\u001b[0m     filename_or_obj,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    546\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    547\u001b[0m ):\n\u001b[0;32m    549\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[1;32m--> 550\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[1;32mC:\\DEV\\Anaconda3\\envs\\xr\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:379\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[1;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[0;32m    373\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    374\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[0;32m    375\u001b[0m )\n\u001b[0;32m    376\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[0;32m    377\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[0;32m    378\u001b[0m )\n\u001b[1;32m--> 379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\DEV\\Anaconda3\\envs\\xr\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:327\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[1;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m--> 327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[1;32mC:\\DEV\\Anaconda3\\envs\\xr\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:388\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\DEV\\Anaconda3\\envs\\xr\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:382\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 382\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[0;32m    383\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[1;32mC:\\DEV\\Anaconda3\\envs\\xr\\lib\\contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[1;32mC:\\DEV\\Anaconda3\\envs\\xr\\lib\\site-packages\\xarray\\backends\\file_manager.py:187\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[1;32mC:\\DEV\\Anaconda3\\envs\\xr\\lib\\site-packages\\xarray\\backends\\file_manager.py:205\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    203\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    204\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[1;32m--> 205\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opener(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overriden when opened again\u001b[39;00m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32msrc\\netCDF4\\_netCDF4.pyx:2333\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\netCDF4\\_netCDF4.pyx:1790\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._get_grps\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\netCDF4\\_netCDF4.pyx:3326\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Group.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\netCDF4\\_netCDF4.pyx:1790\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._get_grps\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\netCDF4\\_netCDF4.pyx:3326\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Group.__init__\u001b[1;34m()\u001b[0m\n",
      "    \u001b[1;31m[... skipping similar frames: netCDF4._netCDF4._get_grps at line 1790 (3 times), netCDF4._netCDF4.Group.__init__ at line 3326 (2 times)]\u001b[0m\n",
      "File \u001b[1;32msrc\\netCDF4\\_netCDF4.pyx:3326\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Group.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\netCDF4\\_netCDF4.pyx:1790\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._get_grps\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\netCDF4\\_netCDF4.pyx:3324\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Group.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\netCDF4\\_netCDF4.pyx:1914\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._get_vars\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\netCDF4\\_netCDF4.pyx:3911\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Variable.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\DEV\\Anaconda3\\envs\\xr\\lib\\site-packages\\netCDF4\\utils.py:34\u001b[0m, in \u001b[0;36m_find_dim\u001b[1;34m(grp, dimname)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sortbylist\u001b[39m(A,B):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# sort one list (A) using the values from another list (B)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [A[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(A)), key\u001b[38;5;241m=\u001b[39mB\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m)]\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_find_dim\u001b[39m(grp, dimname):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# find Dimension instance given group and name.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# look in current group, and parents.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     group \u001b[38;5;241m=\u001b[39m grp\n\u001b[0;32m     38\u001b[0m     dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xr.open_dataset(cfg.PATH_RESOPS / 'raw' / 'timeseries_storage' / 'reservoir_time_series.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68239499-f14a-42da-a761-75486c95da34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf67b4af-4772-4b76-a696-bceed89e6929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f0aa8f1-c265-4dea-8294-a7ca2224bf53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53442489f3e4927a2dd6fc113157c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time series were imported for 59 reservoirs\n"
     ]
    }
   ],
   "source": [
    "# read time series\n",
    "timeseries = {}\n",
    "for sar_id, grand_id in tqdm(map_ana_grand.items()):\n",
    "    file = cfg.PATH_OBS_TS / f'{sar_id}.csv'\n",
    "    if file.is_file():\n",
    "\n",
    "        ts = pd.read_csv(file, parse_dates=['date'], index_col='date')\n",
    "        ts['volume_mcm'] = ts.volume_pct / 100 * attributes.loc[grand_id, 'CAP_MCM']\n",
    "        # make sure there aren't gaps in the dates\n",
    "        dates = pd.date_range(ts.index.min(), ts.index.max(), freq='D')\n",
    "        if len(dates) > ts.shape[0]:\n",
    "            ts = ts.reindex(dates)\n",
    "            ts.index.name = 'date'\n",
    "\n",
    "        # rename columns\n",
    "        rename_cols = {\n",
    "            'volume_mcm': 'storage',\n",
    "            'level_m': 'elevation',\n",
    "            'inflow_cms': 'inflow',\n",
    "            'outflow_cms': 'outflow'\n",
    "        }\n",
    "        ts.rename(columns=rename_cols, inplace=True)\n",
    "        \n",
    "        # clean outliers in storage\n",
    "        clean_storage(ts.storage, w=7, error_thr=.1, inplace=True)\n",
    "        # remove negative values\n",
    "        ts[ts < 0] = np.nan\n",
    "        # clean inflow time series\n",
    "        clean_inflow(\n",
    "            ts.inflow,\n",
    "            storage=ts.storage,\n",
    "            outlfow=ts.outflow,\n",
    "            grad_thr=1e4,\n",
    "            balance_thr=5,\n",
    "            int_method='linear',\n",
    "            inplace=True\n",
    "        )\n",
    "        \n",
    "        # trim time series to period with inflow, storage and outflow\n",
    "        mask_availability = ts[['inflow', 'storage', 'outflow']].notnull().all(axis=1)\n",
    "        if mask_availability.sum() == 0:\n",
    "            continue\n",
    "        start, end = ts[mask_availability].first_valid_index(), ts[mask_availability].last_valid_index()\n",
    "        start, end = max(cfg.START, start), min(cfg.END, end)\n",
    "        attributes.loc[grand_id, ['TIME_SERIES_START', 'TIME_SERIES_END']] = start, end\n",
    "        ts = ts.loc[start:end]\n",
    "        \n",
    "        # save\n",
    "        timeseries[grand_id] = ts.loc[start:end]\n",
    "    else:\n",
    "        print(f'File not found: {file}')\n",
    "print(f'Time series were imported for {len(timeseries)} reservoirs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6526d61-6aab-4696-80ee-7fb74548378b",
   "metadata": {},
   "source": [
    "##### **Plot timeseries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0cea0f8a-e8a4-4fb7-8bae-82eac2c9890e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136a1394cbdb49aaa88c31f15f57ed0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PATH_PLOTS = cfg.PATH_TS / 'plots'\n",
    "PATH_PLOTS.mkdir(exist_ok=True)\n",
    "\n",
    "for grand_id, ts in tqdm(timeseries.items()):\n",
    "    max_storage = {\n",
    "        'GRanD': attributes.loc[grand_id, 'CAP_MCM'],\n",
    "        # 'BR': \n",
    "    }\n",
    "    max_elevation = {\n",
    "        'GRanD': attributes.loc[grand_id, 'ELEV_MASL'],\n",
    "        # 'BR': \n",
    "    }\n",
    "    title = '{0} - {1}'.format(grand_id, attributes.loc[grand_id, 'DAM_NAME'])\n",
    "    plot_timeseries_BR(\n",
    "        ts.storage,\n",
    "        ts.elevation,\n",
    "        ts.outflow,\n",
    "        ts.inflow,\n",
    "        max_storage,\n",
    "        max_elevation,\n",
    "        # zlim=(attributes.loc[grand_id, 'NAME_MASL'] - attributes.loc[grand_id, 'DAM_HGT_M'] * 1.2, None),\n",
    "        title=title,\n",
    "        save=PATH_PLOTS / f'{grand_id}.jpg'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "560a3940-2c9d-4550-8e9a-bfb4e4b61038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grand_id = 1363 #1349 #1347 #1333\n",
    "# ts = timeseries[grand_id]\n",
    "\n",
    "# plot_resops(ts.storage, ts.elevation, outflow=ts.outflow,\n",
    "#             capacity=attributes.loc[grand_id, ['NAME_MCM', 'NAMO_MCM']],\n",
    "#             level=attributes.loc[grand_id, ['NAME_MASL', 'NAMO_MASL']])\n",
    "\n",
    "# plot_resops(ts.storage, ts.area, outflow=ts.outflow,\n",
    "#             capacity=attributes.loc[grand_id, ['NAME_MCM', 'NAMO_MCM']],\n",
    "#             # level=attributes.loc[grand_id, ['NAME_MASL', 'NAMO_MASL']]\n",
    "#            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c53eb1d-d35c-4c92-aedd-79f966374e25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert to xarray.Dataset\n",
    "xarray_list = []\n",
    "for key, df in timeseries.items():\n",
    "    ds = xr.Dataset.from_dataframe(df)\n",
    "    ds = ds.assign_coords(GRAND_ID=key)\n",
    "    xarray_list.append(ds)\n",
    "obs = xr.concat(xarray_list, dim='GRAND_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c98ca-b485-44de-aa1a-3ffd79dad967",
   "metadata": {},
   "source": [
    "#### GloFAS\n",
    "\n",
    "##### Inflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "92a66218-981a-4e73-8c0d-e29bebfe9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import GloFAS simulation\n",
    "# sim = xr.open_dataset(cfg.PATH_SIM_TS / 'dis.nc')\n",
    "# sim = sim.rename({'time': 'date', 'id': 'GRAND_ID', 'dis': 'inflow'})\n",
    "\n",
    "# # bias correct\n",
    "# for grand_id in sim.GRAND_ID.data:\n",
    "    \n",
    "#     if grand_id not in timeseries:\n",
    "#         continue\n",
    "        \n",
    "#     inflow = sim['inflow'].sel(GRAND_ID=grand_id).to_pandas()\n",
    "#     inflow.name = 'inflow'\n",
    "#     ts = timeseries[grand_id]\n",
    "    \n",
    "#     # compute net inflow\n",
    "#     if ('outflow' in ts.columns) & ('storage' in ts.columns):\n",
    "#         ΔS = ts.storage.diff().values\n",
    "#         net_inflow = ΔS * 1e6 / (24 * 3600) + ts.outflow\n",
    "#         net_inflow[net_inflow < 0] = 0\n",
    "#         net_inflow.name = 'net_inflow'\n",
    "\n",
    "#     # bias correct simulated inflow\n",
    "#     inflow_bc = quantile_mapping(obs=net_inflow,\n",
    "#                                  sim=inflow)\n",
    "#     inflow_bc.name = 'inflow_bc'\n",
    "    \n",
    "#     # # plot raw vs bias-corrected inflow\n",
    "#     # compare_flows(ts.storage, ts.outflow, inflow, inflow_bc)\n",
    "    \n",
    "#     # overwrite bias-corrected inflow\n",
    "#     sim['inflow'].loc[{'GRAND_ID': grand_id}] = inflow_bc.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a369b5f-6272-40a9-b989-d5e913473850",
   "metadata": {},
   "source": [
    "##### Meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "455eee68-6de0-4fd0-b7e9-0df260597ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load meteorological time series\n",
    "path_meteo_areal = cfg.PATH_RESOPS / 'ancillary' / 'catchstats' / 'meteo'\n",
    "variables = [x.stem for x in path_meteo_areal.iterdir() if x.is_dir()]\n",
    "meteo_areal = xr.Dataset({f'{var}': xr.open_mfdataset(f'{path_meteo_areal}/{var}/*.nc')[f'{var}_mean'].compute() for var in variables})\n",
    "meteo_areal['time'] = meteo_areal['time'] - np.timedelta64(24, 'h') # WARNING!! One day lag compared with LISFLOOD\n",
    "\n",
    "# keep catchments in the attributes\n",
    "IDs = list(attributes.index.intersection(meteo_areal.id.data))\n",
    "meteo_areal = meteo_areal.sel(id=IDs)\n",
    "\n",
    "# rename 'id' with the GRanD ID\n",
    "meteo_areal = meteo_areal.rename({\n",
    "    'id': 'GRAND_ID',\n",
    "    'time': 'date',\n",
    "    'e0': 'evapo_areal',\n",
    "    'tp': 'precip_areal',\n",
    "    '2t': 'temp_areal'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc5c32d-83b3-4747-a327-bdbd20417914",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "### Convert units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2303dbdc-84b2-4b23-92c5-8a1d674691d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.NORMALIZE:\n",
    "\n",
    "    # reservoir attributes used to normalize the dataset\n",
    "    area_sm = xr.DataArray.from_series(attributes.AREA_SKM) * 1e6 # m2\n",
    "    capacity_cm = xr.DataArray.from_series(attributes.CAP_MCM) * 1e6 # m3\n",
    "    catchment_sm = xr.DataArray.from_series(attributes.CATCH_SKM) * 1e6 # m2\n",
    "    \n",
    "    # Observed timeseries\n",
    "    # -------------------\n",
    "    for var, da in obs.items():\n",
    "        # convert variables in hm3 to fraction of reservoir capacity [-]\n",
    "        if var in ['storage', 'evaporation']:\n",
    "            obs[f'{var}_norm'] = obs[var] * 1e6 / capacity_cm\n",
    "        # convert variables in m3/s to fraction of reservoir capacity [-]\n",
    "        elif var in ['inflow', 'outflow']:\n",
    "            obs[f'{var}_norm'] = obs[var] * 24 * 3600 / capacity_cm\n",
    "\n",
    "#     # Simulated timeseries\n",
    "#     # -------------------\n",
    "#     for var, da in sim.items():\n",
    "#         # convert variables in hm3 to fraction of reservoir capacity [-]\n",
    "#         if var.split('_')[0] in ['storage']:\n",
    "#             sim[f'{var}_norm'] = sim[var] * 1e6 / capacity_cm\n",
    "#         # convert variables in m3/s to fraction of reservoir capacity [-]\n",
    "#         elif var.split('_')[0] in ['inflow', 'outflow']:\n",
    "#             sim[f'{var}_norm'] = sim[var] * 24 * 3600 / capacity_cm\n",
    "            \n",
    "    # Catchment meteorology\n",
    "    # ---------------------\n",
    "    # convert areal evaporation and precipitation from mm to fraction filled\n",
    "    for var in ['evapo', 'precip']:\n",
    "        meteo_areal[f'{var}_areal_norm'] = meteo_areal[f'{var}_areal'] * catchment_sm * 1e-3 / capacity_cm       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ed75f2-62e4-442e-b897-52dee820581e",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "be87f7ec-3b05-4d48-978f-a3bb8ad140a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de93f8e9b9c4f0585e4ae18f1412f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Exporting time series:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_csv = cfg.PATH_TS / 'csv'\n",
    "path_csv.mkdir(parents=True, exist_ok=True)\n",
    "path_nc = cfg.PATH_TS / 'netcdf'\n",
    "path_nc.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for grand_id in tqdm(attributes.index, desc='Exporting time series'):\n",
    "    \n",
    "    if grand_id not in obs.GRAND_ID:\n",
    "        continue\n",
    "        \n",
    "    # concatenate time series\n",
    "    ds = obs.sel(GRAND_ID=grand_id).drop(['GRAND_ID'])\n",
    "    # if grand_id in sim.GRAND_ID.data:\n",
    "    #     ds = xr.merge((ds, sim.sel(GRAND_ID=grand_id).drop(['GRAND_ID'])))\n",
    "    if grand_id in meteo_areal.GRAND_ID.data:\n",
    "        ds = xr.merge((ds, meteo_areal.sel(GRAND_ID=grand_id).drop(['GRAND_ID'])))\n",
    "\n",
    "    # # delete empty variables\n",
    "    # for var in list(ds.data_vars):\n",
    "    #     if (ds[var].isnull().all()):\n",
    "    #         del ds[var]\n",
    "        \n",
    "    # trim time series to the observed period\n",
    "    start, end = attributes.loc[grand_id, ['TIME_SERIES_START', 'TIME_SERIES_END']]\n",
    "    ds = ds.sel(date=slice(start, end))\n",
    "\n",
    "    # create time series of temporal attributes\n",
    "    ds['year'] = ds.date.dt.year\n",
    "    ds['month'] = ds.date.dt.month\n",
    "    ds['month_sin'], ds['month_cos'] = time_encoding(ds['month'], period=12)\n",
    "    ds['weekofyear'] = ds.date.dt.isocalendar().week\n",
    "    ds['woy_sin'], ds['woy_cos'] = time_encoding(ds['weekofyear'], period=52)\n",
    "    ds['dayofyear'] = ds.date.dt.dayofyear\n",
    "    ds['doy_sin'], ds['doy_cos'] = time_encoding(ds['dayofyear'], period=365)\n",
    "    ds['dayofweek'] = ds.date.dt.dayofweek\n",
    "    ds['dow_sin'], ds['dow_cos'] = time_encoding(ds['dayofweek'], period=6)\n",
    "\n",
    "    # export CSV\n",
    "    # ..........\n",
    "    ds.to_pandas().to_csv(path_csv / f'{grand_id}.csv')\n",
    "\n",
    "    # export NetCDF\n",
    "    # .............\n",
    "    ds.to_netcdf(path_nc / f'{grand_id}.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
